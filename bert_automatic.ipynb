{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e180ebcf-4a52-447e-b9c8-0b4a97b6ed39",
   "metadata": {},
   "source": [
    "# Practical classification with pre-trained BERT\n",
    "\n",
    "In this notebook I download pre-trained BERT model and fine-tune it with high-level HuggingFace tools.\n",
    "\n",
    "There is another notebook, doing the same with lower-level PyTorch tools only.\n",
    "\n",
    "## References:\n",
    "* https://huggingface.co/course/chapter3/4?fw=pt - HuggingFace transformers course reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ba3193-2e9a-456a-9f01-bc83f92895f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimal example of using a pre-trained model for classification\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "output = model(**tokens)\n",
    "\n",
    "torch.nn.functional.softmax(output.logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e31ccf8-0fb6-4986-af3a-58dfc755889c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#AUTHID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>cAGR</th>\n",
       "      <th>cCON</th>\n",
       "      <th>cOPN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1997_504851.txt</td>\n",
       "      <td>Well, right now I just woke up from a mid-day ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1997_605191.txt</td>\n",
       "      <td>Well, here we go with the stream of consciousn...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1997_687252.txt</td>\n",
       "      <td>An open keyboard and buttons to push. The thin...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997_568848.txt</td>\n",
       "      <td>I can't believe it!  It's really happening!  M...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1997_688160.txt</td>\n",
       "      <td>Well, here I go with the good old stream of co...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2462</th>\n",
       "      <td>2004_493.txt</td>\n",
       "      <td>I'm home. wanted to go to bed but remembe...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2463</th>\n",
       "      <td>2004_494.txt</td>\n",
       "      <td>Stream of consiousnesssskdj. How do you s...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2464</th>\n",
       "      <td>2004_497.txt</td>\n",
       "      <td>It is Wednesday, December 8th and a lot has be...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2465</th>\n",
       "      <td>2004_498.txt</td>\n",
       "      <td>Man this week has been hellish. Anyways, now i...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>2004_499.txt</td>\n",
       "      <td>I have just gotten off the phone with brady. I...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2467 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              #AUTHID                                               TEXT cEXT  \\\n",
       "0     1997_504851.txt  Well, right now I just woke up from a mid-day ...    0   \n",
       "1     1997_605191.txt  Well, here we go with the stream of consciousn...    0   \n",
       "2     1997_687252.txt  An open keyboard and buttons to push. The thin...    0   \n",
       "3     1997_568848.txt  I can't believe it!  It's really happening!  M...    1   \n",
       "4     1997_688160.txt  Well, here I go with the good old stream of co...    1   \n",
       "...               ...                                                ...  ...   \n",
       "2462     2004_493.txt       I'm home. wanted to go to bed but remembe...    0   \n",
       "2463     2004_494.txt       Stream of consiousnesssskdj. How do you s...    1   \n",
       "2464     2004_497.txt  It is Wednesday, December 8th and a lot has be...    0   \n",
       "2465     2004_498.txt  Man this week has been hellish. Anyways, now i...    0   \n",
       "2466     2004_499.txt  I have just gotten off the phone with brady. I...    0   \n",
       "\n",
       "     cNEU cAGR cCON cOPN  \n",
       "0       1    1    0    1  \n",
       "1       0    1    0    0  \n",
       "2       1    0    1    1  \n",
       "3       0    1    1    0  \n",
       "4       0    1    0    1  \n",
       "...   ...  ...  ...  ...  \n",
       "2462    1    0    1    0  \n",
       "2463    1    0    0    1  \n",
       "2464    0    1    0    0  \n",
       "2465    1    0    0    1  \n",
       "2466    1    1    0    1  \n",
       "\n",
       "[2467 rows x 7 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "essays = pd.read_csv(\"./data/essays.csv\")\n",
    "\n",
    "essays.loc[essays['cEXT'] == 'n', 'cEXT'] = 0\n",
    "essays.loc[essays['cEXT'] == 'y', 'cEXT'] = 1\n",
    "\n",
    "essays.loc[essays['cNEU'] == 'n', 'cNEU'] = 0\n",
    "essays.loc[essays['cNEU'] == 'y', 'cNEU'] = 1\n",
    "\n",
    "essays.loc[essays['cAGR'] == 'n', 'cAGR'] = 0\n",
    "essays.loc[essays['cAGR'] == 'y', 'cAGR'] = 1\n",
    "\n",
    "essays.loc[essays['cCON'] == 'n', 'cCON'] = 0\n",
    "essays.loc[essays['cCON'] == 'y', 'cCON'] = 1\n",
    "\n",
    "essays.loc[essays['cOPN'] == 'n', 'cOPN'] = 0\n",
    "essays.loc[essays['cOPN'] == 'y', 'cOPN'] = 1\n",
    "\n",
    "essays.astype({'cEXT': 'int32', 'cNEU': 'int32', 'cAGR': 'int32', 'cCON': 'int32', 'cOPN': 'int32'}).dtypes\n",
    "\n",
    "essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6f278b6-6155-44e4-a4a5-9e153c68d7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/burkov/Documents/Projects/personal/text2personality/venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|█████████████████████████████████████████████████| 309/309 [00:01<00:00, 203.17ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 10047, 5458, 1010, 13233, 1998, 1045, 2428, 2123, 1005, 1056, 2514, 2035, 2008, 2204, 2157, 2085, 1012, 2026, 4308, 13403, 2021, 10047, 5458, 1012, 1045, 2514, 25227, 1012, 3778, 2013, 7249, 1012, 2082, 1010, 2147, 1010, 2166, 1012, 2129, 2079, 1045, 2113, 2054, 10047, 2725, 2007, 2026, 2166, 2003, 2054, 1045, 2001, 3214, 2000, 2079, 1029, 1045, 2293, 6864, 2016, 2965, 1996, 2088, 2000, 2033, 1012, 1045, 4299, 2008, 1045, 2910, 1005, 1056, 3631, 2039, 2007, 2014, 2197, 2095, 1012, 2009, 9868, 1037, 2843, 1997, 2477, 1999, 2026, 2166, 1012, 2021, 1045, 2245, 2008, 1045, 2052, 2022, 19366, 2007, 2619, 2842, 1998, 1045, 2001, 2005, 1037, 2460, 2558, 1997, 2051, 2021, 2025, 1037, 2154, 2253, 2011, 2008, 1045, 2134, 1005, 1056, 2228, 2055, 6864, 1998, 4687, 2065, 2016, 2003, 2428, 1996, 2028, 1012, 1045, 3335, 2026, 15310, 2040, 2351, 2006, 1996, 2034, 2154, 1997, 2082, 2023, 2095, 1012, 4921, 2063, 2018, 1037, 2428, 7823, 2051, 7149, 2007, 2010, 2331, 1012, 1045, 3984, 1045, 2074, 2245, 2008, 2002, 2052, 2196, 3280, 1012, 2044, 2035, 14472, 4995, 1005, 1056, 4011, 2000, 1006, 1045, 2228, 1007, 1012, 1045, 2074, 2031, 1037, 2524, 2051, 7149, 2007, 2331, 1012, 2021, 22195, 22195, 2003, 2087, 8916, 2135, 1999, 6014, 2157, 2085, 2559, 2091, 2006, 2033, 2437, 2469, 10047, 2542, 2026, 2166, 2004, 1037, 3017, 1012, 1045, 2097, 2156, 2032, 2153, 1010, 1045, 2113, 999, 1996, 2518, 2003, 2008, 22195, 22195, 2428, 3866, 6864, 1998, 2016, 3866, 2032, 1012, 2057, 2020, 2362, 2005, 2471, 2048, 2086, 2077, 1045, 3631, 2039, 2007, 2014, 2005, 14337, 4436, 2029, 1045, 2196, 2179, 2041, 2054, 2027, 2020, 1012, 1045, 5223, 2000, 2022, 2894, 2061, 1045, 2318, 5306, 5553, 2532, 2040, 2003, 1037, 3835, 2611, 2021, 1045, 2354, 2008, 2009, 2001, 2035, 3308, 2021, 1045, 5632, 1996, 11946, 5605, 1010, 2029, 2003, 3308, 2021, 2001, 2204, 2138, 2009, 3271, 2033, 2000, 5382, 2008, 6864, 2003, 1996, 2028, 2005, 2033, 1012, 1996, 2028, 2008, 1045, 2215, 2000, 5247, 1996, 2717, 1997, 2026, 2166, 2007, 1012, 22195, 22195, 2354, 2009, 1012, 2129, 19313, 2009, 2001, 2008, 1045, 3631, 2039, 2007, 5553, 2532, 2069, 2028, 2154, 2077, 22195, 22195, 2351, 1998, 2211, 6224, 2293, 2013, 6864, 1012, 2002, 2052, 2031, 2359, 2149, 2362, 1012, 2021, 1045, 18358, 2039, 1998, 2477, 2024, 2524, 2085, 1010, 2016, 2145, 7459, 2033, 2021, 2016, 2003, 4452, 2000, 3404, 2033, 1012, 1045, 2342, 2014, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split, default_convert\n",
    "from transformers import AdamW, AutoTokenizer, BertForSequenceClassification\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "\n",
    "# prepare dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_function(essays):\n",
    "    return tokenizer(essays[\"TEXT\"], padding=\"max_length\", truncation=True)  # , return_tensors=\"pt\")\n",
    "\n",
    "essays_dataset = Dataset.from_pandas(essays)\n",
    "tokenized_dataset = essays_dataset.map(tokenize_function, batched=True, batch_size=8)\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"TEXT\", \"text\")\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"cNEU\", \"labels\")\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['#AUTHID', 'text', 'cEXT', 'cAGR', 'cCON', 'cOPN'])\n",
    "\n",
    "train_dataset, validation_dataset = random_split(tokenized_dataset, [2000, len(tokenized_dataset) - 2000])\n",
    "\n",
    "ds = DatasetDict()\n",
    "ds['train'] = train_dataset\n",
    "ds['validation'] = validation_dataset\n",
    "\n",
    "# vocab = tokenizer.get_vocab()\n",
    "# ivocab = {v: k for k, v in vocab.items()}\n",
    "print(ds['train'][0]['input_ids'])\n",
    "\n",
    "train_dataloader = DataLoader(ds['train'], shuffle=True, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87512d6c-0a7a-4f62-90fb-0885c5aaa1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2, output_attentions=True)\n",
    "\n",
    "# I'm running this on Apple Silicon. Activate Metal \"mps\" device, if available:\n",
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "\n",
    "else:\n",
    "    mps_device = torch.device(\"mps\")\n",
    "\n",
    "\n",
    "torch.device(\"mps\")\n",
    "model.to(mps_device)\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "616261dd-21d7-4bba-86f4-ca4e9e67d3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████| 250/250 [56:59<00:00, 14.26s/it]"
     ]
    }
   ],
   "source": [
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "# parameters\n",
    "num_epochs = 1  # 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "\n",
    "cross_entropy_loss = torch.nn.CrossEntropyLoss().to(mps_device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "\n",
    "# test on one batch\n",
    "# batch = next(iter(train_dataloader))\n",
    "\n",
    "# labels = batch[\"labels\"]\n",
    "# del batch[\"labels\"]\n",
    "\n",
    "# batch = {k: torch.transpose(torch.stack(default_convert(v)), 0, 1) for k, v in batch.items()}\n",
    "# batch = {k: v.to(mps_device) for k, v in batch.items()}\n",
    "\n",
    "# output = model(**batch)\n",
    "# labels.to(mps_device)\n",
    "# mps_labels = torch.as_tensor(labels, device=mps_device)\n",
    "\n",
    "# loss = cross_entropy_loss(output.logits, mps_labels)\n",
    "# loss.backward()\n",
    "\n",
    "\n",
    "# progress bar\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "# training\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        labels = batch[\"labels\"]\n",
    "        mps_labels = torch.as_tensor(labels, device=mps_device)\n",
    "        del batch[\"labels\"]\n",
    "        \n",
    "        batch = {k: torch.transpose(torch.stack(default_convert(v)), 0, 1) for k, v in batch.items()}\n",
    "        batch = {k: v.to(mps_device) for k, v in batch.items()}\n",
    "\n",
    "        output = model(**batch)\n",
    "\n",
    "        loss = cross_entropy_loss(output.logits, mps_labels)\n",
    "        loss.backward()        \n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae48a77d-033f-43df-9ef8-825082a3bdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs = SequenceClassifierOutput(loss=None, logits=tensor([[0.1740, 0.1375],\n",
      "        [0.1741, 0.1376],\n",
      "        [0.1743, 0.1378],\n",
      "        [0.1740, 0.1377],\n",
      "        [0.1741, 0.1377],\n",
      "        [0.1743, 0.1377],\n",
      "        [0.1741, 0.1377],\n",
      "        [0.1743, 0.1379]], device='mps:0', grad_fn=<MpsLinearBackward0>), hidden_states=None, attentions=None)\n",
      "outputs = tensor([0.4522, 0.5478], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "label = 0\n",
      "outputs = tensor([0.4769, 0.5231], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "label = 1\n",
      "outputs = tensor([0.3628, 0.6372], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "label = 0\n",
      "outputs = tensor([0.4993, 0.5007], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "label = 0\n",
      "outputs = tensor([0.5464, 0.4536], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "label = 0\n",
      "outputs = tensor([0.4529, 0.5471], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "label = 1\n",
      "outputs = tensor([0.5302, 0.4698], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "label = 1\n",
      "outputs = tensor([0.4415, 0.5585], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "label = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v5/07d7bzx52nx37_jlxbw4yq6h0000gn/T/ipykernel_40112/1537677781.py:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(f\"outputs = {softmax(item)}\")\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "# test on one batch\n",
    "batch = next(iter(train_dataloader))\n",
    "\n",
    "labels = batch[\"labels\"]\n",
    "del batch[\"labels\"]\n",
    "\n",
    "batch = {k: torch.transpose(torch.stack(default_convert(v)), 0, 1) for k, v in batch.items()}\n",
    "batch = {k: v.to(mps_device) for k, v in batch.items()}\n",
    "\n",
    "outputs = model(**batch)\n",
    "print(f\"outputs = {outputs}\")\n",
    "\n",
    "for index, item in enumerate(output.logits):\n",
    "    softmax = torch.nn.Softmax()\n",
    "    print(f\"outputs = {softmax(item)}\")\n",
    "    print(f\"label = {labels[index]}\")\n",
    "\n",
    "# labels.to(mps_device)\n",
    "# mps_labels = torch.as_tensor(labels, device=mps_device)\n",
    "\n",
    "# loss = cross_entropy_loss(output.logits, mps_labels)\n",
    "# loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f173b397-1a5e-41d1-8e7e-71b0dc85e61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "\n",
    "metric = load_metric('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb062ff-ce7d-4f03-9699-1e7652dab72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "\n",
    "validation_dataloader = DataLoader(ds['validation'], shuffle=True, batch_size=8)\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "model.eval()\n",
    "for batch in validation_dataloader:\n",
    "    labels = batch[\"labels\"]\n",
    "    mps_labels = torch.as_tensor(labels, device=mps_device)\n",
    "    del batch[\"labels\"]\n",
    "\n",
    "    batch = {k: torch.transpose(torch.stack(default_convert(v)), 0, 1) for k, v in batch.items()}\n",
    "    batch = {k: v.to(mps_device) for k, v in batch.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    softmax = torch.nn.Softmax()\n",
    "    for index, item in enumerate(logits):\n",
    "        print(f\"probabilities = {softmax(item)}\")\n",
    "        print(f\"label = {labels[index]}\")\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=mps_labels)\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3018a55-9078-4235-bf8d-b9d9ff09f3a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\"></script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "      \n",
       "        <div id=\"bertviz-d568b61040194a6fa3146a32c1f9302f\" style=\"font-family:'Helvetica Neue', Helvetica, Arial, sans-serif;\">\n",
       "            <span style=\"user-select:none\">\n",
       "                Layer: <select id=\"layer\"></select>\n",
       "                \n",
       "            </span>\n",
       "            <div id='vis'></div>\n",
       "        </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bertviz import head_view\n",
    "\n",
    "\n",
    "inputs = tokenizer.encode(\"Well, here we go with the stream of consciousness essay. I used to do things like this in high school sometimes. They were pretty interesting, but I often find myself with a lack of things to say. I normally consider myself someone who gets straight to the point. I wonder if I should hit enter any time to send this back to the front. Maybe I'll fix it later. My friend is playing guitar in my room now. Sort of playing anyway. More like messing with it. He's still learning. There's a drawing on the wall next to me. Comic book characters I think, but I'm not sure who they are. It's been a while since I've kept up with comic's. I just heard a sound from ICQ. That's a chat program on the internet. I don't know too much about it so I can't really explain too well. Anyway, I hope I'm done with this by the time another friend comes over. It will be nice to talk to her again. She went home this weekend for Labor Day. So did my brother. I didn't go. I'm not sure why. No reason to go, I guess. Hmm. when did I start this. Wow, that was a long line. I guess I won't change it later. Okay, I'm running out of things to talk about. I've found that happens to me a lot in conversation. Not a very interesting person, I guess.\", return_tensors='pt', truncation=True)\n",
    "inputs = inputs.to(mps_device)\n",
    "outputs = model(inputs)\n",
    "attention = outputs[-1]  # Output includes attention weights when output_attentions=True\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[0])\n",
    "head_view(attention, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63be331d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
