{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49645c52-9c3c-4d52-a9fd-995057d4a086",
   "metadata": {},
   "source": [
    "# Application of BERT to Big-5 prediction from text\n",
    "\n",
    "In this notebook I am re-implementing BERT step by step just for the kicks (then download the pre-trained model from HugginFace). Then I add a head for classification of Big-5 categories.\n",
    "\n",
    "## References:\n",
    "* https://arxiv.org/pdf/1810.04805.pdf - original BERT paper\n",
    "* https://arxiv.org/pdf/1907.11692.pdf - RoBERTa paper\n",
    "* https://openreview.net/pdf?id=rJ4km2R5t7 - GLUE paper\n",
    "* https://neptune.ai/blog/how-to-code-bert-using-pytorch-tutorial - hands-on implementation of BERT in PyTorch step by step\n",
    "* https://pytorch.org/hub/huggingface_pytorch-transformers/ - PyTorch page on transformers\n",
    "* https://habr.com/ru/post/680986/ - review of flavours of BERT (in Russian)\n",
    "* https://discuss.pytorch.org/t/check-if-pytorch-is-using-metal-on-macbook/152481 - on accelerating PyTorch over Metal API on Mac M1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57e5546f-54ac-410c-8dac-18dd1d7fbd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "\n",
    "print(torch.__version__)\n",
    "# print(torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a195cc79-bc1f-4e52-b707-1549ead160c8",
   "metadata": {},
   "source": [
    "## BERT implementation and training\n",
    "\n",
    "First, I'll manually implement BERT architecture in PyTorch to make sure, we have a good feeling of how it works, how to train it etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "1c182d40-9b23-4734-ba33-742195c067be",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\n",
    "   'Hello, how are you? I am Romeo.\\n'\n",
    "   'Hello, Romeo My name is Juliet. Nice to meet you.\\n'\n",
    "   'Nice meet you too. How are you today?\\n'\n",
    "   'Great. My baseball team won the competition.\\n'\n",
    "   'Oh Congratulations, Juliet\\n'\n",
    "   'Thanks you Romeo'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "17cc55e9-123f-4c98-949f-f582c61d218b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello how are you i am romeo', 'hello romeo my name is juliet nice to meet you', 'nice meet you too how are you today', 'great my baseball team won the competition', 'oh congratulations juliet', 'thanks you romeo']\n",
      "['you', 'is', 'my', 'competition', 'won', 'great', 'team', 'hello', 'oh', 'too', 'romeo', 'the', 'i', 'am', 'how', 'meet', 'juliet', 'thanks', 'name', 'are', 'congratulations', 'baseball', 'to', 'today', 'nice']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "# ugly, ugly pre-processing without glmnet\n",
    "sentences = re.sub(\"[.,!?\\\\-]\", '', text.lower()).split('\\n')  # filter '.', ',', '?', '!'\n",
    "word_list = list(set(\" \".join(sentences).split()))\n",
    "print(sentences)\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "4d377580-6dfe-4c7d-8457-0b80bcd4af97",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
    "for i, w in enumerate(word_list):\n",
    "    word_dict[w] = i + 4\n",
    "    \n",
    "number_dict = {i: w for i, w in enumerate(word_dict)}\n",
    "vocab_size = len(word_dict)\n",
    "\n",
    "# calculate maxlen\n",
    "maxlen = 0\n",
    "for sentence in sentences:\n",
    "    if len(sentence.split()) > maxlen:\n",
    "        maxlen = len(sentence)\n",
    "        \n",
    "# build token list\n",
    "token_list = []\n",
    "for sentence in sentences:\n",
    "    token_list.append([])\n",
    "    for token in sentence.split():\n",
    "        token_list[-1].append(word_dict[token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "83e0ceb9-c197-459c-aab6-93256f8dff73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1, 21, 3, 14, 2, 12, 24, 20, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 0, 0, 0, 0, 0, 0, 0, 0, 0], False], [[1, 3, 6, 25, 10, 8, 15, 7, 2, 11, 18, 23, 3, 16, 17, 14, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [7, 9, 4, 0, 0, 0, 0, 0, 0, 0], [7, 1, 12, 0, 0, 0, 0, 0, 0, 0], False], [[1, 9, 6, 25, 10, 8, 15, 7, 2, 3, 18, 23, 4, 16, 3, 24, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [17, 11, 14, 0, 0, 0, 0, 0, 0, 0], [14, 9, 15, 0, 0, 0, 0, 0, 0, 0], False], [[1, 12, 24, 20, 2, 11, 14, 6, 3, 5, 20, 28, 26, 19, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4, 22, 0, 0, 0, 0, 0, 0, 0, 0], [14, 8, 0, 0, 0, 0, 0, 0, 0, 0], False], [[1, 12, 24, 20, 2, 11, 18, 23, 3, 16, 3, 14, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [17, 4, 0, 0, 0, 0, 0, 0, 0, 0], [10, 8, 0, 0, 0, 0, 0, 0, 0, 0], False], [[1, 9, 6, 25, 10, 8, 15, 7, 2, 12, 3, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [20, 24, 0, 0, 0, 0, 0, 0, 0, 0], [11, 10, 0, 0, 0, 0, 0, 0, 0, 0], True], [[1, 11, 18, 23, 4, 16, 17, 14, 2, 3, 6, 25, 3, 8, 17, 7, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [10, 9, 15, 0, 0, 0, 0, 0, 0, 0], [12, 9, 14, 0, 0, 0, 0, 0, 0, 0], False], [[1, 11, 18, 23, 4, 16, 17, 3, 2, 12, 24, 20, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [24, 14, 0, 0, 0, 0, 0, 0, 0, 0], [10, 7, 0, 0, 0, 0, 0, 0, 0, 0], False], [[1, 11, 14, 3, 22, 3, 20, 28, 26, 19, 4, 2, 12, 24, 20, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [5, 6, 0, 0, 0, 0, 0, 0, 0, 0], [5, 3, 0, 0, 0, 0, 0, 0, 0, 0], False], [[1, 21, 4, 3, 2, 12, 24, 20, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [14, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3, 0, 0, 0, 0, 0, 0, 0, 0, 0], False], [[1, 28, 19, 4, 13, 18, 23, 4, 27, 2, 3, 3, 25, 10, 3, 15, 7, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [6, 8, 9, 0, 0, 0, 0, 0, 0, 0], [11, 14, 10, 0, 0, 0, 0, 0, 0, 0], True], [[1, 12, 24, 20, 2, 9, 6, 3, 10, 8, 15, 7, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [7, 25, 0, 0, 0, 0, 0, 0, 0, 0], [11, 7, 0, 0, 0, 0, 0, 0, 0, 0], False], [[1, 21, 4, 14, 2, 11, 14, 6, 22, 5, 20, 28, 3, 19, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [26, 4, 0, 0, 0, 0, 0, 0, 0, 0], [12, 14, 0, 0, 0, 0, 0, 0, 0, 0], False], [[1, 11, 3, 6, 22, 5, 20, 28, 26, 19, 4, 2, 11, 14, 6, 22, 5, 20, 28, 26, 3, 4, 2, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0], [14, 19, 19, 0, 0, 0, 0, 0, 0, 0], [2, 9, 20, 0, 0, 0, 0, 0, 0, 0], False], [[1, 11, 14, 6, 22, 5, 20, 9, 26, 19, 4, 2, 3, 3, 23, 4, 16, 17, 14, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [18, 11, 28, 0, 0, 0, 0, 0, 0, 0], [13, 12, 7, 0, 0, 0, 0, 0, 0, 0], False], [[1, 11, 3, 3, 22, 5, 20, 28, 26, 19, 4, 2, 9, 6, 25, 3, 8, 15, 7, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [6, 10, 14, 0, 0, 0, 0, 0, 0, 0], [3, 15, 2, 0, 0, 0, 0, 0, 0, 0], False], [[1, 9, 6, 25, 12, 8, 15, 7, 2, 28, 19, 4, 13, 18, 3, 4, 27, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [10, 23, 4, 0, 0, 0, 0, 0, 0, 0], [4, 14, 11, 0, 0, 0, 0, 0, 0, 0], False], [[1, 9, 6, 25, 3, 3, 15, 7, 2, 21, 4, 14, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 10, 0, 0, 0, 0, 0, 0, 0, 0], [5, 4, 0, 0, 0, 0, 0, 0, 0, 0], False], [[1, 21, 4, 14, 2, 28, 3, 4, 13, 18, 23, 4, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [19, 27, 0, 0, 0, 0, 0, 0, 0, 0], [6, 12, 0, 0, 0, 0, 0, 0, 0, 0], False], [[1, 28, 19, 4, 13, 18, 23, 4, 27, 2, 9, 6, 25, 10, 8, 3, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [23, 7, 15, 0, 0, 0, 0, 0, 0, 0], [6, 16, 15, 0, 0, 0, 0, 0, 0, 0], True], [[1, 3, 19, 4, 13, 18, 23, 4, 27, 2, 12, 24, 20, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [28, 24, 0, 0, 0, 0, 0, 0, 0, 0], [1, 11, 0, 0, 0, 0, 0, 0, 0, 0], False], [[1, 9, 6, 25, 10, 8, 3, 20, 2, 21, 4, 14, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [15, 7, 0, 0, 0, 0, 0, 0, 0, 0], [6, 7, 0, 0, 0, 0, 0, 0, 0, 0], False], [[1, 11, 18, 23, 4, 16, 17, 14, 2, 28, 19, 4, 3, 18, 3, 4, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [13, 27, 23, 0, 0, 0, 0, 0, 0, 0], [12, 16, 14, 0, 0, 0, 0, 0, 0, 0], False], [[1, 12, 24, 20, 2, 21, 3, 14, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4, 0, 0, 0, 0, 0, 0, 0, 0, 0], [6, 0, 0, 0, 0, 0, 0, 0, 0, 0], True], [[1, 28, 19, 4, 13, 18, 17, 4, 27, 2, 3, 6, 25, 10, 17, 15, 7, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [9, 23, 8, 0, 0, 0, 0, 0, 0, 0], [10, 6, 14, 0, 0, 0, 0, 0, 0, 0], True], [[1, 11, 18, 23, 3, 16, 17, 14, 2, 11, 14, 3, 22, 5, 20, 28, 26, 3, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [4, 19, 6, 0, 0, 0, 0, 0, 0, 0], [4, 17, 11, 0, 0, 0, 0, 0, 0, 0], True], [[1, 28, 19, 4, 3, 18, 23, 4, 27, 2, 21, 4, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [14, 13, 0, 0, 0, 0, 0, 0, 0, 0], [12, 4, 0, 0, 0, 0, 0, 0, 0, 0], False], [[1, 21, 4, 14, 2, 12, 24, 20, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [20, 0, 0, 0, 0, 0, 0, 0, 0, 0], [7, 0, 0, 0, 0, 0, 0, 0, 0, 0], False], [[1, 12, 24, 20, 2, 12, 24, 20, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [24, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 0, 0, 0, 0, 0, 0, 0, 0, 0], False], [[1, 21, 4, 14, 2, 21, 3, 14, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4, 0, 0, 0, 0, 0, 0, 0, 0, 0], [6, 0, 0, 0, 0, 0, 0, 0, 0, 0], False], [[1, 28, 19, 4, 13, 7, 23, 4, 27, 2, 9, 6, 25, 3, 3, 15, 7, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [10, 18, 8, 0, 0, 0, 0, 0, 0, 0], [13, 5, 14, 0, 0, 0, 0, 0, 0, 0], True], [[1, 12, 24, 20, 2, 21, 4, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [14, 0, 0, 0, 0, 0, 0, 0, 0, 0], [7, 0, 0, 0, 0, 0, 0, 0, 0, 0], True], [[1, 11, 3, 6, 22, 5, 20, 28, 26, 3, 4, 2, 28, 19, 4, 13, 18, 23, 3, 27, 2, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [19, 14, 4, 0, 0, 0, 0, 0, 0, 0], [9, 2, 18, 0, 0, 0, 0, 0, 0, 0], True], [[1, 21, 4, 14, 2, 28, 19, 4, 13, 18, 23, 4, 27, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [18, 4, 0, 0, 0, 0, 0, 0, 0, 0], [9, 11, 0, 0, 0, 0, 0, 0, 0, 0], False], [[1, 12, 15, 20, 2, 28, 3, 4, 13, 18, 23, 4, 27, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [24, 19, 0, 0, 0, 0, 0, 0, 0, 0], [2, 6, 0, 0, 0, 0, 0, 0, 0, 0], False], [[1, 21, 4, 3, 2, 11, 18, 23, 4, 3, 17, 14, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [14, 16, 0, 0, 0, 0, 0, 0, 0, 0], [3, 9, 0, 0, 0, 0, 0, 0, 0, 0], False], [[1, 11, 14, 6, 22, 5, 20, 28, 3, 19, 4, 2, 3, 6, 25, 10, 8, 15, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [9, 26, 7, 0, 0, 0, 0, 0, 0, 0], [12, 8, 18, 0, 0, 0, 0, 0, 0, 0], False], [[1, 11, 18, 23, 3, 16, 17, 3, 2, 28, 19, 4, 3, 18, 23, 4, 27, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4, 14, 13, 0, 0, 0, 0, 0, 0, 0], [4, 7, 12, 0, 0, 0, 0, 0, 0, 0], False], [[1, 9, 3, 25, 10, 4, 3, 7, 2, 28, 19, 4, 13, 18, 23, 4, 27, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [6, 15, 8, 0, 0, 0, 0, 0, 0, 0], [2, 6, 5, 0, 0, 0, 0, 0, 0, 0], False], [[1, 11, 14, 6, 22, 5, 20, 28, 26, 19, 3, 2, 12, 3, 20, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4, 24, 0, 0, 0, 0, 0, 0, 0, 0], [10, 13, 0, 0, 0, 0, 0, 0, 0, 0], False], [[1, 3, 24, 20, 2, 12, 24, 20, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [12, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0], False], [[1, 28, 3, 4, 13, 3, 23, 3, 27, 2, 9, 6, 25, 10, 8, 15, 7, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [19, 18, 4, 0, 0, 0, 0, 0, 0, 0], [2, 5, 7, 0, 0, 0, 0, 0, 0, 0], True], [[1, 9, 6, 25, 10, 8, 15, 3, 2, 3, 24, 20, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [7, 12, 0, 0, 0, 0, 0, 0, 0, 0], [7, 9, 0, 0, 0, 0, 0, 0, 0, 0], True], [[1, 12, 24, 20, 2, 21, 3, 14, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4, 0, 0, 0, 0, 0, 0, 0, 0, 0], [6, 0, 0, 0, 0, 0, 0, 0, 0, 0], True], [[1, 28, 19, 4, 13, 3, 23, 3, 27, 2, 9, 6, 25, 3, 8, 15, 7, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [10, 18, 4, 0, 0, 0, 0, 0, 0, 0], [13, 5, 7, 0, 0, 0, 0, 0, 0, 0], True], [[1, 28, 19, 4, 13, 18, 3, 4, 3, 2, 3, 6, 25, 10, 8, 15, 7, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [23, 27, 9, 0, 0, 0, 0, 0, 0, 0], [6, 8, 10, 0, 0, 0, 0, 0, 0, 0], True], [[1, 9, 6, 25, 10, 8, 3, 7, 2, 12, 3, 20, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [15, 24, 0, 0, 0, 0, 0, 0, 0, 0], [6, 10, 0, 0, 0, 0, 0, 0, 0, 0], True], [[1, 9, 3, 25, 10, 3, 15, 7, 2, 12, 24, 20, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 6, 0, 0, 0, 0, 0, 0, 0, 0], [5, 2, 0, 0, 0, 0, 0, 0, 0, 0], True], [[1, 12, 24, 20, 2, 21, 4, 8, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [14, 0, 0, 0, 0, 0, 0, 0, 0, 0], [7, 0, 0, 0, 0, 0, 0, 0, 0, 0], True], [[1, 11, 14, 6, 22, 5, 20, 3, 26, 19, 3, 2, 28, 19, 4, 13, 18, 23, 3, 27, 2, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [28, 4, 4, 0, 0, 0, 0, 0, 0, 0], [7, 18, 10, 0, 0, 0, 0, 0, 0, 0], True], [[1, 3, 24, 20, 2, 21, 4, 14, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [12, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0], True], [[1, 9, 6, 25, 3, 8, 15, 7, 2, 3, 24, 20, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [10, 12, 0, 0, 0, 0, 0, 0, 0, 0], [4, 9, 0, 0, 0, 0, 0, 0, 0, 0], True], [[1, 3, 6, 25, 10, 8, 3, 7, 2, 12, 24, 20, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [15, 9, 0, 0, 0, 0, 0, 0, 0, 0], [6, 1, 0, 0, 0, 0, 0, 0, 0, 0], True], [[1, 11, 14, 3, 22, 3, 3, 28, 26, 19, 4, 2, 28, 19, 4, 13, 18, 23, 4, 27, 2, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [20, 6, 5, 0, 0, 0, 0, 0, 0, 0], [6, 3, 5, 0, 0, 0, 0, 0, 0, 0], True], [[1, 11, 18, 23, 4, 16, 3, 14, 2, 11, 14, 6, 22, 5, 20, 3, 26, 3, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [28, 19, 17, 0, 0, 0, 0, 0, 0, 0], [15, 17, 6, 0, 0, 0, 0, 0, 0, 0], True], [[1, 9, 3, 25, 3, 8, 15, 7, 2, 12, 24, 20, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [6, 10, 0, 0, 0, 0, 0, 0, 0, 0], [2, 4, 0, 0, 0, 0, 0, 0, 0, 0], True], [[1, 25, 14, 6, 22, 5, 20, 28, 26, 19, 4, 2, 3, 3, 4, 13, 18, 23, 4, 27, 2, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [28, 11, 19, 0, 0, 0, 0, 0, 0, 0], [12, 1, 13, 0, 0, 0, 0, 0, 0, 0], True], [[1, 12, 24, 20, 2, 21, 3, 14, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4, 0, 0, 0, 0, 0, 0, 0, 0, 0], [6, 0, 0, 0, 0, 0, 0, 0, 0, 0], True], [[1, 11, 18, 23, 4, 3, 17, 3, 2, 11, 14, 6, 22, 5, 20, 28, 26, 19, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [16, 14, 6, 0, 0, 0, 0, 0, 0, 0], [5, 7, 11, 0, 0, 0, 0, 0, 0, 0], True], [[1, 9, 3, 25, 10, 8, 3, 7, 2, 12, 24, 20, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [6, 15, 0, 0, 0, 0, 0, 0, 0, 0], [2, 6, 0, 0, 0, 0, 0, 0, 0, 0], True], [[1, 11, 18, 23, 3, 16, 17, 14, 2, 3, 14, 6, 22, 5, 3, 28, 26, 19, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [20, 11, 4, 0, 0, 0, 0, 0, 0, 0], [14, 9, 4, 0, 0, 0, 0, 0, 0, 0], True], [[1, 12, 24, 3, 2, 21, 4, 14, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [20, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3, 0, 0, 0, 0, 0, 0, 0, 0, 0], True], [[1, 28, 3, 4, 13, 3, 23, 4, 3, 2, 9, 6, 25, 10, 8, 15, 7, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [19, 18, 27, 0, 0, 0, 0, 0, 0, 0], [2, 5, 8, 0, 0, 0, 0, 0, 0, 0], True], [[1, 11, 3, 23, 4, 16, 17, 14, 2, 11, 14, 6, 3, 5, 20, 28, 26, 19, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [4, 22, 18, 0, 0, 0, 0, 0, 0, 0], [18, 12, 2, 0, 0, 0, 0, 0, 0, 0], True]]\n"
     ]
    }
   ],
   "source": [
    "from random import randrange, shuffle, randint, random\n",
    "\n",
    "\n",
    "def make_batch(\n",
    "        sentences: list,\n",
    "        batch_size: int,\n",
    "        token_list: list,\n",
    "        word_dict: dict,\n",
    "        number_dict: dict,\n",
    "        vocab_size: int,\n",
    "        max_pred: int,\n",
    "        maxlen: int\n",
    "):    \n",
    "    batch = []\n",
    "    positive = negative = 0\n",
    "    while positive != batch_size/2 or negative != batch_size/2:\n",
    "        # pick any two random sentences (not necessarily consecutive)\n",
    "        tokens_a_index, tokens_b_index = randrange(len(sentences)), randrange(len(sentences)) \n",
    "\n",
    "        tokens_a, tokens_b = token_list[tokens_a_index], token_list[tokens_b_index]\n",
    "\n",
    "        # generate input embeddings out of them\n",
    "        input_ids = [word_dict['[CLS]']] + tokens_a + [word_dict['[SEP]']] + tokens_b + [word_dict['[SEP]']]\n",
    "\n",
    "        # mask first sentence with the goal of predicting, whether they are consecutive or not\n",
    "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        # MASK LM\n",
    "        n_pred =  min(max_pred, max(1, int(round(len(input_ids) * 0.15))))  # mask ~15 % of tokens in one sentence\n",
    "        cand_masked_pos = [i for i, token in enumerate(input_ids)\n",
    "                          if token != word_dict['[CLS]'] and token != word_dict['[SEP]']]\n",
    "\n",
    "        shuffle(cand_masked_pos)\n",
    "\n",
    "        masked_tokens, masked_pos = [], []\n",
    "        for pos in cand_masked_pos[:n_pred]:\n",
    "            masked_pos.append(pos)\n",
    "            masked_tokens.append(input_ids[pos])\n",
    "\n",
    "            if random() < 0.8:  # in 80% of the cases, mask the token\n",
    "                input_ids[pos] = word_dict['[MASK]']  # make mask\n",
    "            elif random() < 0.5:  # in 10% of the cases, replace the token with a random token from the vocabulary\n",
    "                index = randint(4, vocab_size - 1)  # random index in vocabulary\n",
    "                input_ids[pos] = index\n",
    "            else:  # in 10% keep the token as is\n",
    "                pass\n",
    "\n",
    "        # Zero Paddings\n",
    "        n_pad = maxlen - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "        segment_ids.extend([0] * n_pad)\n",
    "\n",
    "        # Zero Padding (100% - 15%) tokens\n",
    "        if max_pred > n_pred:\n",
    "            n_pad = max_pred - n_pred\n",
    "            masked_tokens.extend([0] * n_pad)\n",
    "            masked_pos.extend([0] * n_pad)\n",
    "\n",
    "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size/2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True]) # IsNext\n",
    "            positive += 1\n",
    "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size/2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False]) # NotNext\n",
    "            negative += 1\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "batch = make_batch(\n",
    "    sentences=sentences, \n",
    "    batch_size=64, \n",
    "    token_list=token_list, \n",
    "    word_dict=word_dict, \n",
    "    number_dict=number_dict, \n",
    "    vocab_size=vocab_size,\n",
    "    max_pred=10,\n",
    "    maxlen=maxlen\n",
    ")\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "03099daf-6561-48de-8c14-c13f99d079ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL PARAMETERS\n",
    "# ----------------\n",
    "\n",
    "d_model = 256  # parameter, used everywhere below\n",
    "d_ff = 256  # dimensionality of intermediate layer on feed-forward parts of encoder blocks\n",
    "n_segments = 2  # each training data point consists of two sequences, for which we predict, whether they are consecutive\n",
    "d_k = d_model  # in attention heads d_k is the size of W_K * K\n",
    "d_v = d_model  # same idea as d_k, but for attention values\n",
    "n_layers = 6  # number of encoder block layers\n",
    "n_heads = 32  # number of heads per layer, 768 in BERT base "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "70bb2e8c-78f0-47d0-926c-8ce28557fa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding\n",
    "        self.pos_embed = nn.Embedding(maxlen, d_model)  # position embedding\n",
    "        self.seg_embed = nn.Embedding(n_segments, d_model)  # segment(token type) embedding\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        seq_len = x.size(1)\n",
    "        pos = torch.arange(seq_len, dtype=torch.long)\n",
    "        pos = pos.unsqueeze(0).expand_as(x)  # (seq_len,) -> (batch_size, seq_len)\n",
    "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
    "        \n",
    "        return self.norm(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "8e86fe5e-a5b2-4284-9776-c97d9a2afd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "   \n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
    "   \n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "562568e1-a59c-4913-bce8-cb0f53d52e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask)  # enc_inputs to same Q,K,V\n",
    "        enc_outputs = self.pos_ffn(enc_outputs)  # enc_outputs: [batch_size x len_q x d_model]\n",
    "        \n",
    "        return enc_outputs, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "074b4b9a-56b2-48ec-bd4e-514f6d2f1c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "       \n",
    "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
    "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
    "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
    "\n",
    "        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        scores, context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size x len_q x n_heads * d_v]\n",
    "        output = nn.Linear(n_heads * d_v, d_model)(context)\n",
    "\n",
    "        return nn.LayerNorm(d_model)(output + residual), attn # output: [batch_size x len_q x d_model]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "9fae6b1a-372f-4db0-9a64-76fbeeb06559",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    \"\"\"Copy-pasted from: https://github.com/Skumarr53/Attention-is-All-you-Need-PyTorch/blob/master/transformer/model.py\"\"\"\n",
    "    def __init__(self):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.l1 = nn.Linear(d_model, d_ff)\n",
    "        self.l2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "        self.relu = gelu\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        residual = inputs\n",
    "\n",
    "        output = self.l1(inputs)\n",
    "        output = self.relu(output)\n",
    "        output = self.l2(output)\n",
    "\n",
    "        return self.layer_norm(output + residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e17ca2d7-1929-4be4-835a-b1b37aa1414c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V)\n",
    "        \n",
    "        return scores, context, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "1d5637f9-d453-47cf-9df5-36348b4bc0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masks: \n",
      " tensor([[ 4,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 7,  9,  4,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [17, 11, 14,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 4, 22,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [17,  4,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [20, 24,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [10,  9, 15,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [24, 14,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 5,  6,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [14,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 6,  8,  9,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 7, 25,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [26,  4,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [14, 19, 19,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [18, 11, 28,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 6, 10, 14,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [10, 23,  4,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 8, 10,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [19, 27,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [23,  7, 15,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [28, 24,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [15,  7,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [13, 27, 23,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 4,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 9, 23,  8,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 4, 19,  6,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [14, 13,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [20,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [24,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 4,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [10, 18,  8,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [14,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [19, 14,  4,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [18,  4,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [24, 19,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [14, 16,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 9, 26,  7,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 4, 14, 13,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 6, 15,  8,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 4, 24,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [12,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [19, 18,  4,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 7, 12,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 4,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [10, 18,  4,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [23, 27,  9,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [15, 24,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 8,  6,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [14,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [28,  4,  4,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [12,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [10, 12,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [15,  9,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [20,  6,  5,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [28, 19, 17,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 6, 10,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [28, 11, 19,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 4,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [16, 14,  6,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 6, 15,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [20, 11,  4,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [20,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [19, 18, 27,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 4, 22, 18,  0,  0,  0,  0,  0,  0,  0]]) tensor([[ 2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 7,  1, 12,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [14,  9, 15,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [14,  8,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [10,  8,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [11, 10,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [12,  9, 14,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [10,  7,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 5,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [11, 14, 10,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [11,  7,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [12, 14,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  9, 20,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [13, 12,  7,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 3, 15,  2,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 4, 14, 11,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 5,  4,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 6, 12,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 6, 16, 15,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1, 11,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 6,  7,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [12, 16, 14,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 6,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [10,  6, 14,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 4, 17, 11,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [12,  4,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 7,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 6,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [13,  5, 14,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 7,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 9,  2, 18,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 9, 11,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  6,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 3,  9,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [12,  8, 18,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 4,  7, 12,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  6,  5,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [10, 13,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  5,  7,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 7,  9,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 6,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [13,  5,  7,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 6,  8, 10,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 6, 10,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 5,  2,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 7,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 7, 18, 10,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 4,  9,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 6,  1,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 6,  3,  5,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [15, 17,  6,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  4,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [12,  1, 13,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 6,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 5,  7, 11,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  6,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [14,  9,  4,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  5,  8,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [18, 12,  2,  0,  0,  0,  0,  0,  0,  0]])\n",
      "\n",
      "Scores: \n",
      " tensor([ 1.6000e+01,  5.4256e+00,  6.0007e+00,  6.4814e+00,  6.1162e+00,\n",
      "        -3.1573e-01, -2.0336e+00, -1.0725e+00, -1.2955e-02, -1.0000e+09,\n",
      "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "        -1.0000e+09, -1.0000e+09, -1.0000e+09], grad_fn=<SelectBackward0>) \n",
      "\n",
      "Attention Scores after softmax: \n",
      " tensor([9.9980e-01, 2.5559e-05, 4.5424e-05, 7.3461e-05, 5.0985e-05, 8.2055e-08,\n",
      "        1.4724e-08, 3.8498e-08, 1.1107e-07, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
    "\n",
    "emb = Embedding()\n",
    "embeds = emb(input_ids, segment_ids)\n",
    "\n",
    "attenM = get_attn_pad_mask(input_ids, input_ids)\n",
    "\n",
    "SDPA = ScaledDotProductAttention()(embeds, embeds, embeds, attenM)\n",
    "\n",
    "S, C, A = SDPA\n",
    "\n",
    "# print('Masks', masked[0][0])\n",
    "print('Masks: \\n', masked_tokens, masked_pos)\n",
    "print()\n",
    "print('Scores: \\n', S[0][0],'\\n\\nAttention Scores after softmax: \\n', A[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "09d8eeb9-a612-4b1f-9641-ff6abe3a1d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERT, self).__init__()\n",
    "        self.embedding = Embedding()\n",
    "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.activ1 = nn.Tanh()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.activ2 = gelu\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, 2)\n",
    "\n",
    "        # decoder is shared with embedding layer\n",
    "        embed_weight = self.embedding.tok_embed.weight\n",
    "        n_vocab, n_dim = embed_weight.size()\n",
    "        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
    "        self.decoder.weight = embed_weight\n",
    "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, masked_pos):\n",
    "        output = self.embedding(input_ids, segment_ids)\n",
    "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids)\n",
    "        for layer in self.layers:\n",
    "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
    "        # output : [batch_size, len, d_model], attn : [batch_size, n_heads, d_mode, d_model]\n",
    "        # it will be decided by first token(CLS)\n",
    "        h_pooled = self.activ1(self.fc(output[:, 0])) # [batch_size, d_model]\n",
    "        logits_clsf = self.classifier(h_pooled) # [batch_size, 2]\n",
    "\n",
    "        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1)) # [batch_size, max_pred, d_model]\n",
    "\n",
    "        # get masked position from final output of transformer.\n",
    "        h_masked = torch.gather(output, 1, masked_pos) # masking position [batch_size, max_pred, d_model]\n",
    "        h_masked = self.norm(self.activ2(self.linear(h_masked)))\n",
    "        logits_lm = self.decoder(h_masked) + self.decoder_bias # [batch_size, max_pred, n_vocab]\n",
    "\n",
    "        return logits_lm, logits_clsf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "a7d1bda2-ee96-4154-a770-88dca80d035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "d967855a-f9f6-4c62-b4d2-838590e0d267",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "\n",
    "model = BERT()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "0ec5a652-6d6f-48c9-8058-af6479c76570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0010 cost = 1.441227\n",
      "Epoch: 0020 cost = 1.213921\n",
      "Epoch: 0030 cost = 1.255225\n",
      "Epoch: 0040 cost = 1.193672\n",
      "Epoch: 0050 cost = 1.179239\n",
      "Epoch: 0060 cost = 1.156764\n",
      "Epoch: 0070 cost = 1.120726\n",
      "Epoch: 0080 cost = 1.106389\n",
      "Epoch: 0090 cost = 1.079798\n",
      "Epoch: 0100 cost = 1.094847\n",
      "Hello, how are you? I am Romeo.\n",
      "Hello, Romeo My name is Juliet. Nice to meet you.\n",
      "Nice meet you too. How are you today?\n",
      "Great. My baseball team won the competition.\n",
      "Oh Congratulations, Juliet\n",
      "Thanks you Romeo\n",
      "['[CLS]', 'hello', 'how', '[MASK]', 'you', 'i', '[MASK]', 'romeo', '[SEP]', 'thanks', 'you', 'romeo', '[SEP]']\n",
      "masked tokens list :  [23, 17]\n",
      "predict masked tokens list :  [20, 17]\n",
      "isNext :  False\n",
      "predict isNext :  True\n"
     ]
    }
   ],
   "source": [
    "batch = make_batch(\n",
    "    sentences=sentences, \n",
    "    batch_size=64, \n",
    "    token_list=token_list, \n",
    "    word_dict=word_dict, \n",
    "    number_dict=number_dict, \n",
    "    vocab_size=vocab_size,\n",
    "    max_pred=10,\n",
    "    maxlen=maxlen\n",
    ")\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
    "    loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens)  # for masked LM\n",
    "    loss_lm = (loss_lm.float()).mean()\n",
    "    loss_clsf = criterion(logits_clsf, isNext)  # for sentence classification\n",
    "    loss = loss_lm + loss_clsf\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Predict mask tokens\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(batch[0]))\n",
    "print(text)\n",
    "print([number_dict[w.item()] for w in input_ids[0] if number_dict[w.item()] != '[PAD]'])\n",
    "\n",
    "logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
    "logits_lm = logits_lm.data.max(2)[1][0].data.numpy()\n",
    "print('masked tokens list : ', [pos.item() for pos in masked_tokens[0] if pos.item() != 0])\n",
    "print('predict masked tokens list : ', [pos for pos in logits_lm if pos != 0])\n",
    "\n",
    "logits_clsf = logits_clsf.data.max(1)[1].data.numpy()[0]\n",
    "print('isNext : ', True if isNext else False)\n",
    "print('predict isNext : ', True if logits_clsf else False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
