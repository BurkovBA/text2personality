{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49645c52-9c3c-4d52-a9fd-995057d4a086",
   "metadata": {},
   "source": [
    "# Application of BERT to Big-5 prediction from text\n",
    "\n",
    "In this notebook I am re-implementing BERT step by step just for the kicks (then download the pre-trained model from HugginFace). Then I add a head for classification of Big-5 categories.\n",
    "\n",
    "## References:\n",
    "* https://arxiv.org/pdf/1810.04805.pdf - original BERT paper\n",
    "* https://arxiv.org/pdf/1907.11692.pdf - RoBERTa paper\n",
    "* https://openreview.net/pdf?id=rJ4km2R5t7 - GLUE paper\n",
    "* https://neptune.ai/blog/how-to-code-bert-using-pytorch-tutorial - hands-on implementation of BERT in PyTorch step by step\n",
    "* https://pytorch.org/hub/huggingface_pytorch-transformers/ - PyTorch page on transformers\n",
    "* https://habr.com/ru/post/680986/ - review of flavours of BERT (in Russian)\n",
    "* https://discuss.pytorch.org/t/check-if-pytorch-is-using-metal-on-macbook/152481 - on accelerating PyTorch over Metal API on Mac M1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57e5546f-54ac-410c-8dac-18dd1d7fbd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "\n",
    "print(torch.__version__)\n",
    "# print(torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a195cc79-bc1f-4e52-b707-1549ead160c8",
   "metadata": {},
   "source": [
    "## BERT implementation and training\n",
    "\n",
    "First, I'll manually implement BERT architecture in PyTorch to make sure, we have a good feeling of how it works, how to train it etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "1c182d40-9b23-4734-ba33-742195c067be",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\n",
    "   'Hello, how are you? I am Romeo.\\n'\n",
    "   'Hello, Romeo My name is Juliet. Nice to meet you.\\n'\n",
    "   'Nice meet you too. How are you today?\\n'\n",
    "   'Great. My baseball team won the competition.\\n'\n",
    "   'Oh Congratulations, Juliet\\n'\n",
    "   'Thanks you Romeo'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "17cc55e9-123f-4c98-949f-f582c61d218b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello how are you i am romeo', 'hello romeo my name is juliet nice to meet you', 'nice meet you too how are you today', 'great my baseball team won the competition', 'oh congratulations juliet', 'thanks you romeo']\n",
      "['you', 'is', 'my', 'competition', 'won', 'great', 'team', 'hello', 'oh', 'too', 'romeo', 'the', 'i', 'am', 'how', 'meet', 'juliet', 'thanks', 'name', 'are', 'congratulations', 'baseball', 'to', 'today', 'nice']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "# ugly, ugly pre-processing without glmnet\n",
    "sentences = re.sub(\"[.,!?\\\\-]\", '', text.lower()).split('\\n')  # filter '.', ',', '?', '!'\n",
    "word_list = list(set(\" \".join(sentences).split()))\n",
    "print(sentences)\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "4d377580-6dfe-4c7d-8457-0b80bcd4af97",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
    "for i, w in enumerate(word_list):\n",
    "    word_dict[w] = i + 4\n",
    "    \n",
    "number_dict = {i: w for i, w in enumerate(word_dict)}\n",
    "vocab_size = len(word_dict)\n",
    "\n",
    "# calculate maxlen\n",
    "maxlen = 0\n",
    "for sentence in sentences:\n",
    "    if len(sentence.split()) > maxlen:\n",
    "        maxlen = len(sentence)\n",
    "        \n",
    "# build token list\n",
    "token_list = []\n",
    "for sentence in sentences:\n",
    "    token_list.append([])\n",
    "    for token in sentence.split():\n",
    "        token_list[-1].append(word_dict[token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "83e0ceb9-c197-459c-aab6-93256f8dff73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1, 21, 3, 14, 2, 12, 24, 20, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 0, 0, 0, 0, 0, 0, 0, 0, 0], False], [[1, 3, 6, 25, 10, 8, 15, 7, 2, 11, 18, 23, 3, 16, 17, 14, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [7, 9, 4, 0, 0, 0, 0, 0, 0, 0], [7, 1, 12, 0, 0, 0, 0, 0, 0, 0], False], [[1, 9, 6, 25, 10, 8, 15, 7, 2, 3, 18, 23, 4, 16, 3, 24, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [17, 11, 14, 0, 0, 0, 0, 0, 0, 0], [14, 9, 15, 0, 0, 0, 0, 0, 0, 0], False], [[1, 12, 24, 20, 2, 11, 14, 6, 3, 5, 20, 28, 26, 19, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4, 22, 0, 0, 0, 0, 0, 0, 0, 0], [14, 8, 0, 0, 0, 0, 0, 0, 0, 0], False], [[1, 12, 24, 20, 2, 11, 18, 23, 3, 16, 3, 14, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [17, 4, 0, 0, 0, 0, 0, 0, 0, 0], [10, 8, 0, 0, 0, 0, 0, 0, 0, 0], False], [[1, 9, 6, 25, 10, 8, 15, 7, 2, 12, 3, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [20, 24, 0, 0, 0, 0, 0, 0, 0, 0], [11, 10, 0, 0, 0, 0, 0, 0, 0, 0], True], [[1, 11, 18, 23, 4, 16, 17, 14, 2, 3, 6, 25, 3, 8, 17, 7, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [10, 9, 15, 0, 0, 0, 0, 0, 0, 0], [12, 9, 14, 0, 0, 0, 0, 0, 0, 0], False], [[1, 11, 18, 23, 4, 16, 17, 3, 2, 12, 24, 20, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [24, 14, 0, 0, 0, 0, 0, 0, 0, 0], [10, 7, 0, 0, 0, 0, 0, 0, 0, 0], False], [[1, 11, 14, 3, 22, 3, 20, 28, 26, 19, 4, 2, 12, 24, 20, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [5, 6, 0, 0, 0, 0, 0, 0, 0, 0], [5, 3, 0, 0, 0, 0, 0, 0, 0, 0], False], [[1, 21, 4, 3, 2, 12, 24, 20, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [14, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3, 0, 0, 0, 0, 0, 0, 0, 0, 0], False], [[1, 28, 19, 4, 13, 18, 23, 4, 27, 2, 3, 3, 25, 10, 3, 15, 7, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [6, 8, 9, 0, 0, 0, 0, 0, 0, 0], [11, 14, 10, 0, 0, 0, 0, 0, 0, 0], True], [[1, 12, 24, 20, 2, 9, 6, 3, 10, 8, 15, 7, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [7, 25, 0, 0, 0, 0, 0, 0, 0, 0], [11, 7, 0, 0, 0, 0, 0, 0, 0, 0], False], [[1, 21, 4, 14, 2, 11, 14, 6, 22, 5, 20, 28, 3, 19, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [26, 4, 0, 0, 0, 0, 0, 0, 0, 0], [12, 14, 0, 0, 0, 0, 0, 0, 0, 0], False], [[1, 11, 3, 6, 22, 5, 20, 28, 26, 19, 4, 2, 11, 14, 6, 22, 5, 20, 28, 26, 3, 4, 2, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0], [14, 19, 19, 0, 0, 0, 0, 0, 0, 0], [2, 9, 20, 0, 0, 0, 0, 0, 0, 0], False], [[1, 11, 14, 6, 22, 5, 20, 9, 26, 19, 4, 2, 3, 3, 23, 4, 16, 17, 14, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [18, 11, 28, 0, 0, 0, 0, 0, 0, 0], [13, 12, 7, 0, 0, 0, 0, 0, 0, 0], False], [[1, 11, 3, 3, 22, 5, 20, 28, 26, 19, 4, 2, 9, 6, 25, 3, 8, 15, 7, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [6, 10, 14, 0, 0, 0, 0, 0, 0, 0], [3, 15, 2, 0, 0, 0, 0, 0, 0, 0], False], [[1, 9, 6, 25, 12, 8, 15, 7, 2, 28, 19, 4, 13, 18, 3, 4, 27, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [10, 23, 4, 0, 0, 0, 0, 0, 0, 0], [4, 14, 11, 0, 0, 0, 0, 0, 0, 0], False], [[1, 9, 6, 25, 3, 3, 15, 7, 2, 21, 4, 14, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 10, 0, 0, 0, 0, 0, 0, 0, 0], [5, 4, 0, 0, 0, 0, 0, 0, 0, 0], False], [[1, 21, 4, 14, 2, 28, 3, 4, 13, 18, 23, 4, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [19, 27, 0, 0, 0, 0, 0, 0, 0, 0], [6, 12, 0, 0, 0, 0, 0, 0, 0, 0], False], [[1, 28, 19, 4, 13, 18, 23, 4, 27, 2, 9, 6, 25, 10, 8, 3, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [23, 7, 15, 0, 0, 0, 0, 0, 0, 0], [6, 16, 15, 0, 0, 0, 0, 0, 0, 0], True], [[1, 3, 19, 4, 13, 18, 23, 4, 27, 2, 12, 24, 20, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [28, 24, 0, 0, 0, 0, 0, 0, 0, 0], [1, 11, 0, 0, 0, 0, 0, 0, 0, 0], False], [[1, 9, 6, 25, 10, 8, 3, 20, 2, 21, 4, 14, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [15, 7, 0, 0, 0, 0, 0, 0, 0, 0], [6, 7, 0, 0, 0, 0, 0, 0, 0, 0], False], [[1, 11, 18, 23, 4, 16, 17, 14, 2, 28, 19, 4, 3, 18, 3, 4, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [13, 27, 23, 0, 0, 0, 0, 0, 0, 0], [12, 16, 14, 0, 0, 0, 0, 0, 0, 0], False], [[1, 12, 24, 20, 2, 21, 3, 14, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4, 0, 0, 0, 0, 0, 0, 0, 0, 0], [6, 0, 0, 0, 0, 0, 0, 0, 0, 0], True], [[1, 28, 19, 4, 13, 18, 17, 4, 27, 2, 3, 6, 25, 10, 17, 15, 7, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [9, 23, 8, 0, 0, 0, 0, 0, 0, 0], [10, 6, 14, 0, 0, 0, 0, 0, 0, 0], True], [[1, 11, 18, 23, 3, 16, 17, 14, 2, 11, 14, 3, 22, 5, 20, 28, 26, 3, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [4, 19, 6, 0, 0, 0, 0, 0, 0, 0], [4, 17, 11, 0, 0, 0, 0, 0, 0, 0], True], [[1, 28, 19, 4, 3, 18, 23, 4, 27, 2, 21, 4, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [14, 13, 0, 0, 0, 0, 0, 0, 0, 0], [12, 4, 0, 0, 0, 0, 0, 0, 0, 0], False], [[1, 21, 4, 14, 2, 12, 24, 20, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [20, 0, 0, 0, 0, 0, 0, 0, 0, 0], [7, 0, 0, 0, 0, 0, 0, 0, 0, 0], False], [[1, 12, 24, 20, 2, 12, 24, 20, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [24, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 0, 0, 0, 0, 0, 0, 0, 0, 0], False], [[1, 21, 4, 14, 2, 21, 3, 14, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4, 0, 0, 0, 0, 0, 0, 0, 0, 0], [6, 0, 0, 0, 0, 0, 0, 0, 0, 0], False], [[1, 28, 19, 4, 13, 7, 23, 4, 27, 2, 9, 6, 25, 3, 3, 15, 7, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [10, 18, 8, 0, 0, 0, 0, 0, 0, 0], [13, 5, 14, 0, 0, 0, 0, 0, 0, 0], True], [[1, 12, 24, 20, 2, 21, 4, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [14, 0, 0, 0, 0, 0, 0, 0, 0, 0], [7, 0, 0, 0, 0, 0, 0, 0, 0, 0], True], [[1, 11, 3, 6, 22, 5, 20, 28, 26, 3, 4, 2, 28, 19, 4, 13, 18, 23, 3, 27, 2, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [19, 14, 4, 0, 0, 0, 0, 0, 0, 0], [9, 2, 18, 0, 0, 0, 0, 0, 0, 0], True], [[1, 21, 4, 14, 2, 28, 19, 4, 13, 18, 23, 4, 27, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [18, 4, 0, 0, 0, 0, 0, 0, 0, 0], [9, 11, 0, 0, 0, 0, 0, 0, 0, 0], False], [[1, 12, 15, 20, 2, 28, 3, 4, 13, 18, 23, 4, 27, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [24, 19, 0, 0, 0, 0, 0, 0, 0, 0], [2, 6, 0, 0, 0, 0, 0, 0, 0, 0], False], [[1, 21, 4, 3, 2, 11, 18, 23, 4, 3, 17, 14, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [14, 16, 0, 0, 0, 0, 0, 0, 0, 0], [3, 9, 0, 0, 0, 0, 0, 0, 0, 0], False], [[1, 11, 14, 6, 22, 5, 20, 28, 3, 19, 4, 2, 3, 6, 25, 10, 8, 15, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [9, 26, 7, 0, 0, 0, 0, 0, 0, 0], [12, 8, 18, 0, 0, 0, 0, 0, 0, 0], False], [[1, 11, 18, 23, 3, 16, 17, 3, 2, 28, 19, 4, 3, 18, 23, 4, 27, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4, 14, 13, 0, 0, 0, 0, 0, 0, 0], [4, 7, 12, 0, 0, 0, 0, 0, 0, 0], False], [[1, 9, 3, 25, 10, 4, 3, 7, 2, 28, 19, 4, 13, 18, 23, 4, 27, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [6, 15, 8, 0, 0, 0, 0, 0, 0, 0], [2, 6, 5, 0, 0, 0, 0, 0, 0, 0], False], [[1, 11, 14, 6, 22, 5, 20, 28, 26, 19, 3, 2, 12, 3, 20, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4, 24, 0, 0, 0, 0, 0, 0, 0, 0], [10, 13, 0, 0, 0, 0, 0, 0, 0, 0], False], [[1, 3, 24, 20, 2, 12, 24, 20, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [12, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0], False], [[1, 28, 3, 4, 13, 3, 23, 3, 27, 2, 9, 6, 25, 10, 8, 15, 7, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [19, 18, 4, 0, 0, 0, 0, 0, 0, 0], [2, 5, 7, 0, 0, 0, 0, 0, 0, 0], True], [[1, 9, 6, 25, 10, 8, 15, 3, 2, 3, 24, 20, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [7, 12, 0, 0, 0, 0, 0, 0, 0, 0], [7, 9, 0, 0, 0, 0, 0, 0, 0, 0], True], [[1, 12, 24, 20, 2, 21, 3, 14, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4, 0, 0, 0, 0, 0, 0, 0, 0, 0], [6, 0, 0, 0, 0, 0, 0, 0, 0, 0], True], [[1, 28, 19, 4, 13, 3, 23, 3, 27, 2, 9, 6, 25, 3, 8, 15, 7, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [10, 18, 4, 0, 0, 0, 0, 0, 0, 0], [13, 5, 7, 0, 0, 0, 0, 0, 0, 0], True], [[1, 28, 19, 4, 13, 18, 3, 4, 3, 2, 3, 6, 25, 10, 8, 15, 7, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [23, 27, 9, 0, 0, 0, 0, 0, 0, 0], [6, 8, 10, 0, 0, 0, 0, 0, 0, 0], True], [[1, 9, 6, 25, 10, 8, 3, 7, 2, 12, 3, 20, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [15, 24, 0, 0, 0, 0, 0, 0, 0, 0], [6, 10, 0, 0, 0, 0, 0, 0, 0, 0], True], [[1, 9, 3, 25, 10, 3, 15, 7, 2, 12, 24, 20, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 6, 0, 0, 0, 0, 0, 0, 0, 0], [5, 2, 0, 0, 0, 0, 0, 0, 0, 0], True], [[1, 12, 24, 20, 2, 21, 4, 8, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [14, 0, 0, 0, 0, 0, 0, 0, 0, 0], [7, 0, 0, 0, 0, 0, 0, 0, 0, 0], True], [[1, 11, 14, 6, 22, 5, 20, 3, 26, 19, 3, 2, 28, 19, 4, 13, 18, 23, 3, 27, 2, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [28, 4, 4, 0, 0, 0, 0, 0, 0, 0], [7, 18, 10, 0, 0, 0, 0, 0, 0, 0], True], [[1, 3, 24, 20, 2, 21, 4, 14, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [12, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0], True], [[1, 9, 6, 25, 3, 8, 15, 7, 2, 3, 24, 20, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [10, 12, 0, 0, 0, 0, 0, 0, 0, 0], [4, 9, 0, 0, 0, 0, 0, 0, 0, 0], True], [[1, 3, 6, 25, 10, 8, 3, 7, 2, 12, 24, 20, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [15, 9, 0, 0, 0, 0, 0, 0, 0, 0], [6, 1, 0, 0, 0, 0, 0, 0, 0, 0], True], [[1, 11, 14, 3, 22, 3, 3, 28, 26, 19, 4, 2, 28, 19, 4, 13, 18, 23, 4, 27, 2, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [20, 6, 5, 0, 0, 0, 0, 0, 0, 0], [6, 3, 5, 0, 0, 0, 0, 0, 0, 0], True], [[1, 11, 18, 23, 4, 16, 3, 14, 2, 11, 14, 6, 22, 5, 20, 3, 26, 3, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [28, 19, 17, 0, 0, 0, 0, 0, 0, 0], [15, 17, 6, 0, 0, 0, 0, 0, 0, 0], True], [[1, 9, 3, 25, 3, 8, 15, 7, 2, 12, 24, 20, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [6, 10, 0, 0, 0, 0, 0, 0, 0, 0], [2, 4, 0, 0, 0, 0, 0, 0, 0, 0], True], [[1, 25, 14, 6, 22, 5, 20, 28, 26, 19, 4, 2, 3, 3, 4, 13, 18, 23, 4, 27, 2, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [28, 11, 19, 0, 0, 0, 0, 0, 0, 0], [12, 1, 13, 0, 0, 0, 0, 0, 0, 0], True], [[1, 12, 24, 20, 2, 21, 3, 14, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4, 0, 0, 0, 0, 0, 0, 0, 0, 0], [6, 0, 0, 0, 0, 0, 0, 0, 0, 0], True], [[1, 11, 18, 23, 4, 3, 17, 3, 2, 11, 14, 6, 22, 5, 20, 28, 26, 19, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [16, 14, 6, 0, 0, 0, 0, 0, 0, 0], [5, 7, 11, 0, 0, 0, 0, 0, 0, 0], True], [[1, 9, 3, 25, 10, 8, 3, 7, 2, 12, 24, 20, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [6, 15, 0, 0, 0, 0, 0, 0, 0, 0], [2, 6, 0, 0, 0, 0, 0, 0, 0, 0], True], [[1, 11, 18, 23, 3, 16, 17, 14, 2, 3, 14, 6, 22, 5, 3, 28, 26, 19, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [20, 11, 4, 0, 0, 0, 0, 0, 0, 0], [14, 9, 4, 0, 0, 0, 0, 0, 0, 0], True], [[1, 12, 24, 3, 2, 21, 4, 14, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [20, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3, 0, 0, 0, 0, 0, 0, 0, 0, 0], True], [[1, 28, 3, 4, 13, 3, 23, 4, 3, 2, 9, 6, 25, 10, 8, 15, 7, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [19, 18, 27, 0, 0, 0, 0, 0, 0, 0], [2, 5, 8, 0, 0, 0, 0, 0, 0, 0], True], [[1, 11, 3, 23, 4, 16, 17, 14, 2, 11, 14, 6, 3, 5, 20, 28, 26, 19, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [4, 22, 18, 0, 0, 0, 0, 0, 0, 0], [18, 12, 2, 0, 0, 0, 0, 0, 0, 0], True]]\n"
     ]
    }
   ],
   "source": [
    "from random import randrange, shuffle, randint, random\n",
    "\n",
    "\n",
    "def make_batch(\n",
    "        sentences: list,\n",
    "        batch_size: int,\n",
    "        token_list: list,\n",
    "        word_dict: dict,\n",
    "        number_dict: dict,\n",
    "        vocab_size: int,\n",
    "        max_pred: int,\n",
    "        maxlen: int\n",
    "):    \n",
    "    batch = []\n",
    "    positive = negative = 0\n",
    "    while positive != batch_size/2 or negative != batch_size/2:\n",
    "        # pick any two random sentences (not necessarily consecutive)\n",
    "        tokens_a_index, tokens_b_index = randrange(len(sentences)), randrange(len(sentences)) \n",
    "\n",
    "        tokens_a, tokens_b = token_list[tokens_a_index], token_list[tokens_b_index]\n",
    "\n",
    "        # generate input embeddings out of them\n",
    "        input_ids = [word_dict['[CLS]']] + tokens_a + [word_dict['[SEP]']] + tokens_b + [word_dict['[SEP]']]\n",
    "\n",
    "        # mask first sentence with the goal of predicting, whether they are consecutive or not\n",
    "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        # MASK LM\n",
    "        n_pred =  min(max_pred, max(1, int(round(len(input_ids) * 0.15))))  # mask ~15 % of tokens in one sentence\n",
    "        cand_masked_pos = [i for i, token in enumerate(input_ids)\n",
    "                          if token != word_dict['[CLS]'] and token != word_dict['[SEP]']]\n",
    "\n",
    "        shuffle(cand_masked_pos)\n",
    "\n",
    "        masked_tokens, masked_pos = [], []\n",
    "        for pos in cand_masked_pos[:n_pred]:\n",
    "            masked_pos.append(pos)\n",
    "            masked_tokens.append(input_ids[pos])\n",
    "\n",
    "            if random() < 0.8:  # in 80% of the cases, mask the token\n",
    "                input_ids[pos] = word_dict['[MASK]']  # make mask\n",
    "            elif random() < 0.5:  # in 10% of the cases, replace the token with a random token from the vocabulary\n",
    "                index = randint(4, vocab_size - 1)  # random index in vocabulary\n",
    "                input_ids[pos] = index\n",
    "            else:  # in 10% keep the token as is\n",
    "                pass\n",
    "\n",
    "        # Zero Paddings\n",
    "        n_pad = maxlen - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "        segment_ids.extend([0] * n_pad)\n",
    "\n",
    "        # Zero Padding (100% - 15%) tokens\n",
    "        if max_pred > n_pred:\n",
    "            n_pad = max_pred - n_pred\n",
    "            masked_tokens.extend([0] * n_pad)\n",
    "            masked_pos.extend([0] * n_pad)\n",
    "\n",
    "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size/2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True]) # IsNext\n",
    "            positive += 1\n",
    "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size/2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False]) # NotNext\n",
    "            negative += 1\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "batch = make_batch(\n",
    "    sentences=sentences, \n",
    "    batch_size=64, \n",
    "    token_list=token_list, \n",
    "    word_dict=word_dict, \n",
    "    number_dict=number_dict, \n",
    "    vocab_size=vocab_size,\n",
    "    max_pred=10,\n",
    "    maxlen=maxlen\n",
    ")\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "03099daf-6561-48de-8c14-c13f99d079ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL PARAMETERS\n",
    "# ----------------\n",
    "\n",
    "d_model = 256  # parameter, used everywhere below\n",
    "d_ff = 256  # dimensionality of intermediate layer on feed-forward parts of encoder blocks\n",
    "n_segments = 2  # each training data point consists of two sequences, for which we predict, whether they are consecutive\n",
    "d_k = d_model  # in attention heads d_k is the size of W_K * K\n",
    "d_v = d_model  # same idea as d_k, but for attention values\n",
    "n_layers = 6  # number of encoder block layers\n",
    "n_heads = 32  # number of heads per layer, 768 in BERT base "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "70bb2e8c-78f0-47d0-926c-8ce28557fa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding\n",
    "        self.pos_embed = nn.Embedding(maxlen, d_model)  # position embedding\n",
    "        self.seg_embed = nn.Embedding(n_segments, d_model)  # segment(token type) embedding\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        seq_len = x.size(1)\n",
    "        pos = torch.arange(seq_len, dtype=torch.long)\n",
    "        pos = pos.unsqueeze(0).expand_as(x)  # (seq_len,) -> (batch_size, seq_len)\n",
    "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
    "        \n",
    "        return self.norm(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "8e86fe5e-a5b2-4284-9776-c97d9a2afd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "   \n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
    "   \n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "562568e1-a59c-4913-bce8-cb0f53d52e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask)  # enc_inputs to same Q,K,V\n",
    "        enc_outputs = self.pos_ffn(enc_outputs)  # enc_outputs: [batch_size x len_q x d_model]\n",
    "        \n",
    "        return enc_outputs, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "074b4b9a-56b2-48ec-bd4e-514f6d2f1c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "       \n",
    "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
    "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
    "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
    "\n",
    "        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        scores, context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size x len_q x n_heads * d_v]\n",
    "        output = nn.Linear(n_heads * d_v, d_model)(context)\n",
    "\n",
    "        return nn.LayerNorm(d_model)(output + residual), attn # output: [batch_size x len_q x d_model]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "9fae6b1a-372f-4db0-9a64-76fbeeb06559",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    \"\"\"Copy-pasted from: https://github.com/Skumarr53/Attention-is-All-you-Need-PyTorch/blob/master/transformer/model.py\"\"\"\n",
    "    def __init__(self):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.l1 = nn.Linear(d_model, d_ff)\n",
    "        self.l2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "        self.relu = gelu\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        residual = inputs\n",
    "\n",
    "        output = self.l1(inputs)\n",
    "        output = self.relu(output)\n",
    "        output = self.l2(output)\n",
    "\n",
    "        return self.layer_norm(output + residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e17ca2d7-1929-4be4-835a-b1b37aa1414c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V)\n",
    "        \n",
    "        return scores, context, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "1d5637f9-d453-47cf-9df5-36348b4bc0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masks: \n",
      " tensor([[ 4,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 7,  9,  4,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [17, 11, 14,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 4, 22,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [17,  4,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [20, 24,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [10,  9, 15,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [24, 14,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 5,  6,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [14,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 6,  8,  9,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 7, 25,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [26,  4,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [14, 19, 19,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [18, 11, 28,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 6, 10, 14,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [10, 23,  4,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 8, 10,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [19, 27,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [23,  7, 15,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [28, 24,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [15,  7,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [13, 27, 23,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 4,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 9, 23,  8,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 4, 19,  6,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [14, 13,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [20,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [24,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 4,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [10, 18,  8,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [14,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [19, 14,  4,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [18,  4,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [24, 19,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [14, 16,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 9, 26,  7,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 4, 14, 13,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 6, 15,  8,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 4, 24,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [12,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [19, 18,  4,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 7, 12,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 4,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [10, 18,  4,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [23, 27,  9,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [15, 24,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 8,  6,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [14,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [28,  4,  4,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [12,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [10, 12,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [15,  9,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [20,  6,  5,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [28, 19, 17,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 6, 10,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [28, 11, 19,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 4,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [16, 14,  6,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 6, 15,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [20, 11,  4,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [20,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [19, 18, 27,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 4, 22, 18,  0,  0,  0,  0,  0,  0,  0]]) tensor([[ 2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 7,  1, 12,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [14,  9, 15,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [14,  8,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [10,  8,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [11, 10,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [12,  9, 14,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [10,  7,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 5,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [11, 14, 10,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [11,  7,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [12, 14,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  9, 20,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [13, 12,  7,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 3, 15,  2,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 4, 14, 11,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 5,  4,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 6, 12,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 6, 16, 15,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1, 11,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 6,  7,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [12, 16, 14,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 6,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [10,  6, 14,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 4, 17, 11,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [12,  4,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 7,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 6,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [13,  5, 14,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 7,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 9,  2, 18,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 9, 11,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  6,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 3,  9,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [12,  8, 18,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 4,  7, 12,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  6,  5,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [10, 13,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  5,  7,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 7,  9,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 6,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [13,  5,  7,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 6,  8, 10,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 6, 10,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 5,  2,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 7,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 7, 18, 10,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 4,  9,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 6,  1,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 6,  3,  5,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [15, 17,  6,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  4,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [12,  1, 13,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 6,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 5,  7, 11,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  6,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [14,  9,  4,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  5,  8,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [18, 12,  2,  0,  0,  0,  0,  0,  0,  0]])\n",
      "\n",
      "Scores: \n",
      " tensor([ 1.6000e+01,  5.4256e+00,  6.0007e+00,  6.4814e+00,  6.1162e+00,\n",
      "        -3.1573e-01, -2.0336e+00, -1.0725e+00, -1.2955e-02, -1.0000e+09,\n",
      "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "        -1.0000e+09, -1.0000e+09, -1.0000e+09], grad_fn=<SelectBackward0>) \n",
      "\n",
      "Attention Scores after softmax: \n",
      " tensor([9.9980e-01, 2.5559e-05, 4.5424e-05, 7.3461e-05, 5.0985e-05, 8.2055e-08,\n",
      "        1.4724e-08, 3.8498e-08, 1.1107e-07, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
    "\n",
    "emb = Embedding()\n",
    "embeds = emb(input_ids, segment_ids)\n",
    "\n",
    "attenM = get_attn_pad_mask(input_ids, input_ids)\n",
    "\n",
    "SDPA = ScaledDotProductAttention()(embeds, embeds, embeds, attenM)\n",
    "\n",
    "S, C, A = SDPA\n",
    "\n",
    "# print('Masks', masked[0][0])\n",
    "print('Masks: \\n', masked_tokens, masked_pos)\n",
    "print()\n",
    "print('Scores: \\n', S[0][0],'\\n\\nAttention Scores after softmax: \\n', A[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "09d8eeb9-a612-4b1f-9641-ff6abe3a1d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERT, self).__init__()\n",
    "        self.embedding = Embedding()\n",
    "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.activ1 = nn.Tanh()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.activ2 = gelu\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, 2)\n",
    "\n",
    "        # decoder is shared with embedding layer\n",
    "        embed_weight = self.embedding.tok_embed.weight\n",
    "        n_vocab, n_dim = embed_weight.size()\n",
    "        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
    "        self.decoder.weight = embed_weight\n",
    "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, masked_pos):\n",
    "        output = self.embedding(input_ids, segment_ids)\n",
    "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids)\n",
    "        for layer in self.layers:\n",
    "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
    "        # output : [batch_size, len, d_model], attn : [batch_size, n_heads, d_mode, d_model]\n",
    "        # it will be decided by first token(CLS)\n",
    "        h_pooled = self.activ1(self.fc(output[:, 0])) # [batch_size, d_model]\n",
    "        logits_clsf = self.classifier(h_pooled) # [batch_size, 2]\n",
    "\n",
    "        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1)) # [batch_size, max_pred, d_model]\n",
    "\n",
    "        # get masked position from final output of transformer.\n",
    "        h_masked = torch.gather(output, 1, masked_pos) # masking position [batch_size, max_pred, d_model]\n",
    "        h_masked = self.norm(self.activ2(self.linear(h_masked)))\n",
    "        logits_lm = self.decoder(h_masked) + self.decoder_bias # [batch_size, max_pred, n_vocab]\n",
    "\n",
    "        return logits_lm, logits_clsf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "a7d1bda2-ee96-4154-a770-88dca80d035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "d967855a-f9f6-4c62-b4d2-838590e0d267",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "\n",
    "model = BERT()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "0ec5a652-6d6f-48c9-8058-af6479c76570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0010 cost = 1.441227\n",
      "Epoch: 0020 cost = 1.213921\n",
      "Epoch: 0030 cost = 1.255225\n",
      "Epoch: 0040 cost = 1.193672\n",
      "Epoch: 0050 cost = 1.179239\n",
      "Epoch: 0060 cost = 1.156764\n",
      "Epoch: 0070 cost = 1.120726\n",
      "Epoch: 0080 cost = 1.106389\n",
      "Epoch: 0090 cost = 1.079798\n",
      "Epoch: 0100 cost = 1.094847\n",
      "Hello, how are you? I am Romeo.\n",
      "Hello, Romeo My name is Juliet. Nice to meet you.\n",
      "Nice meet you too. How are you today?\n",
      "Great. My baseball team won the competition.\n",
      "Oh Congratulations, Juliet\n",
      "Thanks you Romeo\n",
      "['[CLS]', 'hello', 'how', '[MASK]', 'you', 'i', '[MASK]', 'romeo', '[SEP]', 'thanks', 'you', 'romeo', '[SEP]']\n",
      "masked tokens list :  [23, 17]\n",
      "predict masked tokens list :  [20, 17]\n",
      "isNext :  False\n",
      "predict isNext :  True\n"
     ]
    }
   ],
   "source": [
    "batch = make_batch(\n",
    "    sentences=sentences, \n",
    "    batch_size=64, \n",
    "    token_list=token_list, \n",
    "    word_dict=word_dict, \n",
    "    number_dict=number_dict, \n",
    "    vocab_size=vocab_size,\n",
    "    max_pred=10,\n",
    "    maxlen=maxlen\n",
    ")\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
    "    loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens)  # for masked LM\n",
    "    loss_lm = (loss_lm.float()).mean()\n",
    "    loss_clsf = criterion(logits_clsf, isNext)  # for sentence classification\n",
    "    loss = loss_lm + loss_clsf\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Predict mask tokens\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(batch[0]))\n",
    "print(text)\n",
    "print([number_dict[w.item()] for w in input_ids[0] if number_dict[w.item()] != '[PAD]'])\n",
    "\n",
    "logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
    "logits_lm = logits_lm.data.max(2)[1][0].data.numpy()\n",
    "print('masked tokens list : ', [pos.item() for pos in masked_tokens[0] if pos.item() != 0])\n",
    "print('predict masked tokens list : ', [pos for pos in logits_lm if pos != 0])\n",
    "\n",
    "logits_clsf = logits_clsf.data.max(1)[1].data.numpy()[0]\n",
    "print('isNext : ', True if isNext else False)\n",
    "print('predict isNext : ', True if logits_clsf else False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14601eae-aa0b-438d-b2c0-edabef615011",
   "metadata": {},
   "source": [
    "## Practical classification with pre-trained BERT\n",
    "\n",
    "Now that we're done studying how BERT training works, let us proceed by downloading a pre-trained BERT model from hugging face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b167ee3-3a24-49b3-9c42-3b62fcfde03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class BERTClassification(nn.Module):\n",
    "    def __init__ (self):\n",
    "        # for reference on bert outputs for classification see:\n",
    "        # https://huggingface.co/transformers/v3.0.2/model_doc/bert.html#bertmodel\n",
    "        # https://stackoverflow.com/questions/61331991/bert-pooled-output-is-different-from-first-vector-of-sequence-output\n",
    "        super(BERTClassification, self).__init__()\n",
    "        self.bert = transformers.BertModel.from_pretrained('bert-base-cased')\n",
    "        self.bert_dropout = nn.Dropout(p=0.4)\n",
    "        self.classifier = nn.Linear(768, 1)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        sequence_output, pooled_output = self.bert(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids, \n",
    "            return_dict=False\n",
    "        )\n",
    "        bert_with_dropout = self.bert_dropout(pooled_output)\n",
    "        output = self.classifier(bert_with_dropout)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "688f43d4-af45-454c-a1a6-d8433ffaab24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "\n",
    "model = BERTClassification()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "928b9765-6e51-41aa-919a-c6fc95435c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#AUTHID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>cAGR</th>\n",
       "      <th>cCON</th>\n",
       "      <th>cOPN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1997_504851.txt</td>\n",
       "      <td>Well, right now I just woke up from a mid-day ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1997_605191.txt</td>\n",
       "      <td>Well, here we go with the stream of consciousn...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1997_687252.txt</td>\n",
       "      <td>An open keyboard and buttons to push. The thin...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997_568848.txt</td>\n",
       "      <td>I can't believe it!  It's really happening!  M...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1997_688160.txt</td>\n",
       "      <td>Well, here I go with the good old stream of co...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2462</th>\n",
       "      <td>2004_493.txt</td>\n",
       "      <td>I'm home. wanted to go to bed but remembe...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2463</th>\n",
       "      <td>2004_494.txt</td>\n",
       "      <td>Stream of consiousnesssskdj. How do you s...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2464</th>\n",
       "      <td>2004_497.txt</td>\n",
       "      <td>It is Wednesday, December 8th and a lot has be...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2465</th>\n",
       "      <td>2004_498.txt</td>\n",
       "      <td>Man this week has been hellish. Anyways, now i...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>2004_499.txt</td>\n",
       "      <td>I have just gotten off the phone with brady. I...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2467 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              #AUTHID                                               TEXT cEXT  \\\n",
       "0     1997_504851.txt  Well, right now I just woke up from a mid-day ...    0   \n",
       "1     1997_605191.txt  Well, here we go with the stream of consciousn...    0   \n",
       "2     1997_687252.txt  An open keyboard and buttons to push. The thin...    0   \n",
       "3     1997_568848.txt  I can't believe it!  It's really happening!  M...    1   \n",
       "4     1997_688160.txt  Well, here I go with the good old stream of co...    1   \n",
       "...               ...                                                ...  ...   \n",
       "2462     2004_493.txt       I'm home. wanted to go to bed but remembe...    0   \n",
       "2463     2004_494.txt       Stream of consiousnesssskdj. How do you s...    1   \n",
       "2464     2004_497.txt  It is Wednesday, December 8th and a lot has be...    0   \n",
       "2465     2004_498.txt  Man this week has been hellish. Anyways, now i...    0   \n",
       "2466     2004_499.txt  I have just gotten off the phone with brady. I...    0   \n",
       "\n",
       "     cNEU cAGR cCON cOPN  \n",
       "0       1    1    0    1  \n",
       "1       0    1    0    0  \n",
       "2       1    0    1    1  \n",
       "3       0    1    1    0  \n",
       "4       0    1    0    1  \n",
       "...   ...  ...  ...  ...  \n",
       "2462    1    0    1    0  \n",
       "2463    1    0    0    1  \n",
       "2464    0    1    0    0  \n",
       "2465    1    0    0    1  \n",
       "2466    1    1    0    1  \n",
       "\n",
       "[2467 rows x 7 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "essays = pd.read_csv(\"./data/essays.csv\")\n",
    "\n",
    "essays.loc[essays['cEXT'] == 'n', 'cEXT'] = 0\n",
    "essays.loc[essays['cEXT'] == 'y', 'cEXT'] = 1\n",
    "\n",
    "essays.loc[essays['cNEU'] == 'n', 'cNEU'] = 0\n",
    "essays.loc[essays['cNEU'] == 'y', 'cNEU'] = 1\n",
    "\n",
    "essays.loc[essays['cAGR'] == 'n', 'cAGR'] = 0\n",
    "essays.loc[essays['cAGR'] == 'y', 'cAGR'] = 1\n",
    "\n",
    "essays.loc[essays['cCON'] == 'n', 'cCON'] = 0\n",
    "essays.loc[essays['cCON'] == 'y', 'cCON'] = 1\n",
    "\n",
    "essays.loc[essays['cOPN'] == 'n', 'cOPN'] = 0\n",
    "essays.loc[essays['cOPN'] == 'y', 'cOPN'] = 1\n",
    "\n",
    "essays.astype({'cEXT': 'int32', 'cNEU': 'int32', 'cAGR': 'int32', 'cCON': 'int32', 'cOPN': 'int32'}).dtypes\n",
    "\n",
    "essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a85964d-da5c-4882-9b9b-4833be01be05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y, test_X, test_y = essays['TEXT'][:2000], essays['cEXT'][:2000], essays['TEXT'][2000:], essays['cEXT'][2000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dffd8ae3-43ab-4f09-ad41-22bcd09eab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "essays_training_dataset = Dataset.from_pandas(essays[:2000])\n",
    "essays_validation_dataset = Dataset.from_pandas(essays[2000:])\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# def preprocess_function(examples):\n",
    "#     return tokenizer(examples['TEXT'], truncation=True)\n",
    "\n",
    "tokenized_essays = tokenizer(essays_training_dataset['TEXT'], padding=True, truncation=True, return_tensors=\"pt\")  # essays_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "8dee25d0-3762-432b-9874-4f5210019c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b2e2d9-5d0b-4c57-a622-57827571ddd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# automated training via Trainer; to be used later\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=essays_training_dataset,\n",
    "    eval_dataset=essays_validation_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a81b7958-8a69-451d-8dab-2c2e928b77c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (bert_dropout): Dropout(p=0.4, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I'm running this on Apple Silicon. Activate Metal \"mps\" device, if available:\n",
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "\n",
    "else:\n",
    "    mps_device = torch.device(\"mps\")\n",
    "\n",
    "\n",
    "torch.device(\"mps\")\n",
    "model.to(mps_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "672d59aa-ef2a-4054-86bb-3ddddbfce3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                                                                                                             | 0/78 [00:00<?, ?ba/s]\u001b[A\n",
      " 10%|                                                                                                                                                                                | 8/78 [00:00<00:00, 78.54ba/s]\u001b[A\n",
      " 22%|                                                                                                                                                         | 17/78 [00:00<00:00, 82.73ba/s]\u001b[A\n",
      " 33%|                                                                                                                                  | 26/78 [00:00<00:00, 78.93ba/s]\u001b[A\n",
      " 44%|                                                                                                              | 34/78 [00:00<00:00, 79.06ba/s]\u001b[A\n",
      " 54%|                                                                                          | 42/78 [00:00<00:00, 78.79ba/s]\u001b[A\n",
      " 64%|                                                                      | 50/78 [00:00<00:00, 76.25ba/s]\u001b[A\n",
      " 74%|                                                  | 58/78 [00:00<00:00, 74.02ba/s]\u001b[A\n",
      " 85%|                              | 66/78 [00:00<00:00, 72.11ba/s]\u001b[A\n",
      "100%|| 78/78 [00:01<00:00, 74.96ba/s]\u001b[A\n",
      "\n",
      "  0%|                                                                                                                                                                                                            | 0/234 [02:54<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['labels', 'input_ids', 'token_type_ids', 'attention_mask'])\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (512) to match target batch_size (32).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 48>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m     56\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[0;32m---> 57\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     60\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Documents/Projects/personal/text2personality/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Projects/personal/text2personality/venv/lib/python3.9/site-packages/torch/nn/modules/loss.py:1164\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projects/personal/text2personality/venv/lib/python3.9/site-packages/torch/nn/functional.py:3014\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3012\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3013\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3014\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (512) to match target batch_size (32)."
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, random_split, default_convert\n",
    "from datasets import Dataset\n",
    "from transformers import get_scheduler\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"TEXT\"], padding=\"max_length\", truncation=True)  # , return_tensors=\"pt\")\n",
    "\n",
    "essays_dataset = Dataset.from_pandas(essays)\n",
    "tokenized_dataset = essays_dataset.map(tokenize_function, batched=True, batch_size=32)\n",
    "\n",
    "train_dataset, test_dataset = random_split(tokenized_dataset, [2000, len(tokenized_dataset) - 2000])\n",
    "\n",
    "# train_dataset = train_dataset.shuffle(seed=42).select(range(1000))\n",
    "# test_dataset = test_dataset.shuffle(seed=42).select(range(1000))\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"TEXT\", \"text\")\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"cNEU\", \"labels\")\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['#AUTHID', 'text', 'cEXT', 'cAGR', 'cCON', 'cOPN'])\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_dataset, shuffle=True, batch_size=32)\n",
    "\n",
    "# parameters\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "\n",
    "# optimizer, scheduler, loss, etc.\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", \n",
    "    optimizer=optimizer, \n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        print(batch.keys())\n",
    "        labels = batch[\"labels\"]\n",
    "        del batch[\"labels\"]\n",
    "        batch = {k: torch.stack(default_convert(v)) for k, v in batch.items()}\n",
    "        batch = {k: v.to(mps_device) for k, v in batch.items()}\n",
    "        print(batch.keys())\n",
    "        outputs = model(**batch)\n",
    "        loss = cross_entropy_loss(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009943e9-28c9-4097-96f7-6bd1286d1a92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
